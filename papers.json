[
    {
        "log_id": "001",
        "filename": "001-graph-slam.html",
        "title": "A Tutorial on Graph-Based SLAM",
        "authors": "Giorgio Grisetti, Rainer K\u00fcmmerle, Cyrill Stachniss, Wolfram Burgard",
        "publication": "IEEE Intelligent Transportation Systems Magazine (2010)",
        "tags": [
            "SLAM",
            "Graph Optimization",
            "Tutorial",
            "Backend"
        ],
        "summary_problem": "SLAM (Simultaneous Localization and Mapping) requires optimizing robot poses and landmark positions jointly. Traditional approaches using filtering methods suffer from computational complexity and inconsistent estimates, especially in large-scale environments with loop closures.",
        "summary_solution": "The paper presents a comprehensive tutorial on graph-based SLAM, where the problem is formulated as a graph optimization. Nodes represent robot poses, edges represent spatial constraints from odometry or loop closures. The goal is to find the configuration of nodes that best satisfies all constraints, typically solved using nonlinear least squares optimization.",
        "summary_result": "Graph-based SLAM provides a unified framework that is computationally efficient, scalable to large environments, and produces globally consistent maps. The tutorial covers frontend (data association) and backend (optimization) components, with clear explanations of key algorithms like Gauss-Newton and Levenberg-Marquardt optimization.",
        "strengths": [
            "Clear Tutorial: Provides an accessible introduction to graph-based SLAM with mathematical foundations.",
            "Efficient Backend: Graph optimization is more efficient than filtering methods for large-scale SLAM.",
            "Global Consistency: Produces globally consistent maps by optimizing all poses simultaneously.",
            "Modular Framework: Separates frontend (data association) from backend (optimization)."
        ],
        "weaknesses": [
            "Requires Loop Closure: Performance heavily depends on correct loop closure detection.",
            "Sparse Representation: Typically produces sparse maps unless combined with dense mapping.",
            "Initial Guess Dependency: Optimization requires good initial estimates to converge properly."
        ],
        "score_math": "95",
        "score_code": "70",
        "score_hw": "30",
        "score_novelty": "75",
        "radar_points": "50,12.0 78.0,50 50,62.0 20.0,50",
        "dataset": "Intel Research Lab, MIT Killian Court",
        "method_tech": "Graph Optimization, Least Squares, Gauss-Newton, Levenberg-Marquardt",
        "video_id": "",
        "image_file": "001-graph.png",
        "image_caption": "Figure 1: Graph-based SLAM representation",
        "pdf_file": "001-graph-slam.pdf",
        "bibtex": "@article{grisetti2010tutorial,\n  title={A tutorial on graph-based SLAM},\n  author={Grisetti, Giorgio and K{\\\"u}mmerle, Rainer and Stachniss, Cyrill and Burgard, Wolfram},\n  journal={IEEE Intelligent Transportation Systems Magazine},\n  volume={2},\n  number={4},\n  pages={31--43},\n  year={2010},\n  publisher={IEEE}\n}"
    },
    {
        "log_id": "002",
        "filename": "002-orb-slam.html",
        "title": "ORB-SLAM: a Versatile and Accurate Monocular SLAM System",
        "authors": "Ra\u00fal Mur-Artal, J. M. M. Montiel, Juan D. Tard\u00f3s",
        "publication": "IEEE Transactions on Robotics (2015)",
        "tags": [
            "Visual SLAM",
            "ORB Features",
            "Real-time",
            "Open Source"
        ],
        "summary_problem": "Previous monocular SLAM systems, such as PTAM, were limited to small-scale operations, lacked effective loop closing, and struggled with relocalization from different viewpoints. Furthermore, existing initialization methods often required human intervention or assumed planar scenes, while filtering approaches wasted computation on redundant frames.",
        "summary_solution": "The authors propose a comprehensive system that uses ORB features for all tasks: tracking, mapping, relocalization, and loop closing, ensuring efficiency and invariance to viewpoint. The system introduces a \"survival of the fittest\" strategy for keyframe selection and an Essential Graph to perform real-time loop closing and optimization",
        "summary_result": "ORB-SLAM achieves unprecedented accuracy and robustness across 27 sequences from major datasets (TUM, KITTI, NewCollege), outperforming state-of-the-art direct methods like LSD-SLAM. It operates in real-time on standard CPUs without GPU acceleration and provides a fully open-source implementation.",
        "strengths": [
            "Unified Feature System: Using ORB features for tracking, mapping, and place recognition simplifies the architecture and boosts efficiency.",
            "Robust Initialization: Features an automatic initialization step that intelligently selects between homography (planar) and fundamental matrix (non-planar) models.",
            "Lifelong Mapping: An aggressive keyframe culling policy removes redundant data, allowing the map to scale with scene content rather than time.",
            "Efficient Optimization: The use of an Essential Graph (a sparse subgraph of the covisibility graph) enables accurate and fast loop closing."
        ],
        "weaknesses": [
            "Sparse Reconstruction: As a feature-based method, it produces a sparse point cloud map, which is less informative for tasks like obstacle avoidance compared to dense or semi-dense maps from direct methods.",
            "Texture Dependency: The system relies on corner features (FAST), making it potentially less robust in low-texture environments compared to direct methods that use pixel intensities.",
            "Scale Ambiguity: Like all pure monocular systems, the absolute scale of the world is unobservable and can drift without specific correction mechanisms."
        ],
        "score_math": "85",
        "score_code": "95",
        "score_hw": "40",
        "score_novelty": "90",
        "radar_points": "50,16.0 88.0,50 50,66.0 14.0,50",
        "dataset": "NewCollege, TUM RGB-D, KITTI",
        "method_tech": "Bundle Adjustment, ORB Features, Covisibility Graph, Bag of Words (DBoW2).",
        "video_id": "8p54FkZ8gq4",
        "image_file": "002-arc.png",
        "image_caption": "Figure 1. ORB-SLAM system overview",
        "pdf_file": "002-annotated.pdf",
        "bibtex": "@article{mur2015orb,\r\n  title={ORB-SLAM: a versatile and accurate monocular SLAM system},\r\n  author={Mur-Artal, Ra{\\'u}l and Montiel, Jose Maria Martinez and Tard{\\'o}s, Juan D},\r\n  journal={IEEE Transactions on Robotics},\r\n  volume={31},\r\n  number={5},\r\n  pages={1147--1163},\r\n  year={2015},\r\n  publisher={IEEE}\r\n}"
    },
    {
        "log_id": "003",
        "filename": "003-lm-nav.html",
        "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action",
        "authors": "Dhruv Shah, B\u0142a\u017cej Osi\u0144ski, Brian Ichter, Sergey Levine",
        "publication": "arXiv (2022) / Robotics at Google",
        "tags": [
            "VLM",
            "LLM",
            "Navigation",
            "Zero-Shot",
            "Robot Learning"
        ],
        "summary_problem": "Existing robotic navigation methods typically require expensive language-annotated datasets for training or rely on unnatural interfaces like goal images or GPS coordinates. There is no efficient way to utilize natural language instructions without heavy fine-tuning or manual annotation",
        "summary_solution": "The authors propose LM-Nav, a system that combines three large pre-trained models without any fine-tuning: GPT-3 (LLM) to parse text instructions into landmarks, CLIP (VLM) to ground these landmarks in visual observations, and ViNG (VNM) for visual navigation control. A probabilistic graph search is used to find the optimal path connecting these components.",
        "summary_result": "The system successfully navigates a robot in complex outdoor environments (e.g., campus parks) using free-form natural language instructions. It can traverse hundreds of meters and disambiguate routes based on detailed commands, all without ever seeing labeled language-navigation data before.",
        "strengths": [
            "Zero-Shot / No Fine-Tuning: The system is constructed entirely from off-the-shelf pre-trained models (frozen weights), eliminating the need for re-training or expensive robot annotation data.",
            "Modular Architecture: The clear separation between language (LLM), vision (VLM), and navigation (VNM) modules allows each component to be upgraded independently.",
            "Robust Navigation: By using a self-supervised VNM (ViNG), the physical navigation is more robust to obstacles compared to pure GPS-based methods, as shown in ablation studies."
        ],
        "weaknesses": [
            "Landmark Reliance: The system heavily relies on landmark detection (nouns) and tends to disregard verbs or specific action commands (e.g., \"walk slowly\", \"avoid the dog\").",
            "VLM Limitations: Navigation capabilities are bounded by the VLM's (CLIP) ability to recognize specific objects. The paper notes detection failures for objects like fire hydrants, which can lead to routing errors"
        ],
        "algorithm_logic": [
            "// CORE_LOGIC: PROBABILISTIC_GRAPH_SEARCH",
            "1. INPUT_PROCESSING:",
            "- Instructions -> LLM (GPT-3) -> Landmarks List (L)",
            "- Observations -> VNM (ViNG) -> Topological Graph G(V,E)",
            "2. GROUNDING_DISTRIBUTION (Pre-computation):",
            "FOR EACH Node v in G AND Landmark l in L:",
            "- P_ground(v|l) = VLM_CLIP(Image_v, Text_l)",
            "3. OPTIMAL_WALK_SEARCH (Dynamic Programming):",
            "Initialize Q[0, StartNode] = 0",
            "FOR i = 1 TO num_landmarks:",
            "FOR EACH Node v in G:",
            "# Maximize prob of grounding - travel cost",
            "Q[i,v] = max(Q[i-1, u] + log(P_ground) - dist(u,v))",
            "4. EXECUTION:",
            "- Path = Backtrack(argmax(Q[end]))",
            "- Robot_Action = VNM_Controller(Path)"
        ],
        "performance_benchmark": [
            {
                "metric": "Nav Success Rate (EnvSmall)",
                "ours": "80%",
                "competitor": "23%"
            },
            {
                "metric": "Disengagement Rate (Lower=Better)",
                "ours": "0.1",
                "competitor": "0.75"
            },
            {
                "metric": "Landmark Detection (VLM)",
                "ours": "87%",
                "competitor": "38%"
            },
            {
                "metric": "Instruction Parsing (LLM)",
                "ours": "100%",
                "competitor": "76%"
            },
            {
                "metric": "Planning Efficiency",
                "ours": "0.96",
                "competitor": "0.93"
            }
        ],
        "score_math": "70",
        "score_code": "85",
        "score_hw": "60",
        "score_novelty": "95",
        "radar_points": "50,22.0 84.0,50 50,74.0 12.0,50",
        "dataset": "Self-supervised navigation data (ViNG), no language-robot dataset.",
        "method_tech": "LLM (GPT-3) + VLM (CLIP) + VNM (ViNG) + Graph Search.",
        "video_id": "-",
        "image_file": "003-arch.png",
        "image_caption": "",
        "pdf_file": "003-annotated.pdf",
        "bibtex": "@article{shah2022lmnav,\r\n  title={LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action},\r\n  author={Shah, Dhruv and Osi\u0144ski, B\u0142a\u017cej and Ichter, Brian and Levine, Sergey},\r\n  journal={arXiv preprint arXiv:2207.04429},\r\n  year={2022}\r\n}"
    }
]